{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPt25AFksEH+QXD6bLSnczV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install langchain_core"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_SstJNqD4wF","executionInfo":{"status":"ok","timestamp":1768237079189,"user_tz":-330,"elapsed":6062,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"c9ca27c0-4d4c-4a19-df1b-a7a06c9d5118"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.7)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n","Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.59)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.12.3)\n","Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.2)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n","Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.12.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.5)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (2.32.5)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (4.12.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2.5.0)\n"]}]},{"cell_type":"code","source":["!pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U75vFW68EHX6","executionInfo":{"status":"ok","timestamp":1768236124396,"user_tz":-330,"elapsed":358402,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"29a0782a-047f-4ff0-efe2-5916db0271b2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","Y\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","Y\n","y\n","Y\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4422309 sha256=cb84023261dcdf19990805f326d610cad3f559ad00a3f5705ef10cdf8a6191c8\n","  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2syGSHoVDFrv","executionInfo":{"status":"ok","timestamp":1768237554332,"user_tz":-330,"elapsed":4,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"outputs":[],"source":["import langchain, langchain_community, langchain_core, langchain_classic"]},{"cell_type":"code","source":["from langchain_community.llms import LlamaCpp\n","from huggingface_hub import hf_hub_download\n","\n","# Make sure the model path is correct for your system!\n","# Download the model from Hugging Face\n","model_name = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n","model_file = \"Phi-3-mini-4k-instruct-q4.gguf\"\n","model_path = hf_hub_download(repo_id=model_name, filename=model_file)\n","\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    n_gpu_layers=-1,\n","    max_tokens=500,\n","    n_ctx=2048,\n","    seed=42,\n","    verbose=False\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pjil8efVDKeS","executionInfo":{"status":"ok","timestamp":1768236725798,"user_tz":-330,"elapsed":13106,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"611fdc37-5b7b-4c9d-8a60-b7af1570ab5d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n","llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"]}]},{"cell_type":"code","source":["llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"VN_9GaK8Dbr2","executionInfo":{"status":"ok","timestamp":1768236733694,"user_tz":-330,"elapsed":7192,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"64913143-9a94-4aff-979d-e43e2502f837"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import langchain_community\n","from langchain_core.prompts import PromptTemplate\n","\n","# Create a prompt template with the \"input_prompt\" variable\n","template = \"\"\"<s><|user|>\n","{input_prompt}\n","<|end|>\n","<|assistant|>\"\"\"\n","prompt = PromptTemplate(\n","template=template,\n","input_variables=[\"input_prompt\"]\n",")"],"metadata":{"id":"9-sU54WSGhF4","executionInfo":{"status":"ok","timestamp":1768237102535,"user_tz":-330,"elapsed":521,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["basic_chain = prompt | llm"],"metadata":{"id":"Ky73S1YrHXnR","executionInfo":{"status":"ok","timestamp":1768237123423,"user_tz":-330,"elapsed":710,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Use the chain\n","basic_chain.invoke(\n","{\n","\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n","}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"h8KC5SsrJUGG","executionInfo":{"status":"ok","timestamp":1768237154043,"user_tz":-330,"elapsed":21715,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"b7f0277b-d7dd-4a25-eb2b-3e1e18893235"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["\" Hello Maarten! The answer to 1 + 1 is 2. It's a basic arithmetic addition problem where you combine one unit with another, resulting in two units in total.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from langchain_classic.chains import LLMChain\n","# Create a chain for the title of our story\n","template = \"\"\"<s><|user|>\n","Create a title for a story about\n","{summary}\n",". Only return the title.\n","<|end|>\n","<|assistant|>\"\"\"\n","title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n","title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZP_ID8GJWU5","executionInfo":{"status":"ok","timestamp":1768237615025,"user_tz":-330,"elapsed":1002,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"5a5a79a7-477f-4e26-f1ce-dd6da7b7e74e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3287247481.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n","  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"]}]},{"cell_type":"code","source":["title.invoke({\"summary\": \"a girl that lost her mother\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dj49yHTKniW","executionInfo":{"status":"ok","timestamp":1768237641780,"user_tz":-330,"elapsed":15667,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"935c480e-bc7b-4761-eef3-99ccb7c1dc42"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'summary': 'a girl that lost her mother',\n"," 'title': ' \"Whispers of a Mother\\'s Love: A Journey Through Grief\"'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Create a chain for the character description using the summary and title\n","template = \"\"\"<s><|user|>\n","Describe the main character of a story about\n","{summary}\n"," with the\n","title\n","{title}\n",". Use only two sentences.<|end|>\n","<|assistant|>\"\"\"\n","character_prompt = PromptTemplate(\n","template=template, input_variables=[\"summary\", \"title\"]\n",")\n","character = LLMChain(llm=llm, prompt=character_prompt,\n","output_key=\"character\")\n"],"metadata":{"id":"jRkUuvyuLO4c","executionInfo":{"status":"ok","timestamp":1768237721548,"user_tz":-330,"elapsed":3,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Create a chain for the story using the summary, title, and character description\n","template = \"\"\"<s><|user|>\n","Create a story about\n","{summary}\n"," with the title\n","{title}\n",". The main\n","character is:\n","{character}\n",". Only return the story and it cannot be\n","longer than one paragraph. <|end|>\n","<|assistant|>\"\"\"\n","story_prompt = PromptTemplate(\n","template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",")\n","story = LLMChain(llm=llm, prompt=story_prompt,\n","output_key=\"story\")"],"metadata":{"id":"7_KwB6gDLkpJ","executionInfo":{"status":"ok","timestamp":1768237776945,"user_tz":-330,"elapsed":433,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Combine all three components to create the full chain\n","llm_chain = title | character | story"],"metadata":{"id":"HexvwZgpLzjR","executionInfo":{"status":"ok","timestamp":1768237803610,"user_tz":-330,"elapsed":528,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["llm_chain.invoke(\"a girl that lost her mother\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHD6eI8JL6Hm","executionInfo":{"status":"ok","timestamp":1768237995978,"user_tz":-330,"elapsed":174067,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"76bb398c-88e9-4fc1-caa2-51e43ca75db9"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'summary': 'a girl that lost her mother',\n"," 'title': ' \"A Mother\\'s Absence: Navigating Life Without a Guiding Light\"',\n"," 'character': \" The protagonist, Amelia, is a resilient and compassionate teenage girl grappling with the overwhelming void left by her mother's sudden passing. As she navigates life without her guiding light, Amelia discovers inner strength, forges new connections, and learns to find solace in cherished memories of her beloved mother.\",\n"," 'story': \" Title: A Mother's Absence: Navigating Life Without a Guiding Light\\n\\nAmelia, a resilient and compassionate teenage girl, struggled to come to terms with the sudden loss of her mother. With each passing day, she felt an overwhelming void that seemed impossible to fill. But Amelia's determination to honor her mother's memory pushed her forward in a world without her guiding light. She discovered inner strength and learned to forge new connections as friends and mentors stepped into the role her mother once held. Through cherished memories, she found solace and realized that while her mother may no longer be physically present, her love and guidance would forever remain an integral part of Amelia's life.\"}"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":[],"metadata":{"id":"9CO1LcB4L-st"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's give the LLM our name\n","basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QZucUbxDMffH","executionInfo":{"status":"ok","timestamp":1768238068691,"user_tz":-330,"elapsed":20966,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"7a8774cd-77b5-4b5e-949c-d93a698af1f5"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" Hello Maarten! The answer to 1 + 1 is 2. It's a basic arithmetic addition where one unit added to another unit equals two units in total.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# Next, we ask the LLM to reproduce the name\n","basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"QWPFjJw2Moj-","executionInfo":{"status":"ok","timestamp":1768238096971,"user_tz":-330,"elapsed":28285,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"cc065753-c51f-4fca-c7d6-2c9d2208d7dd"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" I'm sorry, but as an AI, I don't have the ability to know personal information about individuals unless it has been shared with me in the course of our conversation. If you're looking for help identifying a name related to your context or needs, feel free to provide more details!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Create an updated prompt template to include a chat history\n","template = \"\"\"<s><|user|>Current conversation:\n","{chat_history}\n","{input_prompt}\n","<|end|>\n","<|assistant|>\"\"\"\n","prompt = PromptTemplate(\n","template=template,\n","input_variables=[\"input_prompt\", \"chat_history\"]\n",")\n"],"metadata":{"id":"4GvWWIwFM2sg","executionInfo":{"status":"ok","timestamp":1768238175082,"user_tz":-330,"elapsed":4,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["from langchain_classic.memory import ConversationBufferMemory\n","\n","# Define the type of memory we will use\n","memory = ConversationBufferMemory(memory_key=\"chat_history\")\n","\n","# Chain the LLM, prompt, and memory together\n","llm_chain = LLMChain(\n","prompt=prompt,\n","llm=llm,\n","memory=memory\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJJtnsxANUyp","executionInfo":{"status":"ok","timestamp":1768238367698,"user_tz":-330,"elapsed":438,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"36e30636-d15f-4318-d424-0d6d919e03f9"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1786352283.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"]}]},{"cell_type":"code","source":["# Generate a conversation and ask a basic question\n","llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEXie4wqNr84","executionInfo":{"status":"ok","timestamp":1768238424455,"user_tz":-330,"elapsed":25725,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"ad533172-c9c0-4145-baef-f70557c46724"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n"," 'chat_history': '',\n"," 'text': \" The answer to 1 + 1 is 2. Hello, Maarten! It's a simple arithmetic question where when you add one unit to another unit, you get two units in total.\"}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# Does the LLM remember the name we gave it?\n","llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okHR2BQBOLgp","executionInfo":{"status":"ok","timestamp":1768238473194,"user_tz":-330,"elapsed":33821,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"2099b044-bf5f-45be-bf34-1efcce47efbd"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'What is my name?',\n"," 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. Hello, Maarten! It's a simple arithmetic question where when you add one unit to another unit, you get two units in total.\",\n"," 'text': \" Your name is Assistant. Nice to meet you, Maarten! I'm here to help with any questions or tasks you need assistance with.\"}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["llm_chain.invoke({\"input_prompt\": \"What did the conversation we have till now?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcbeTJc0OVcE","executionInfo":{"status":"ok","timestamp":1768238584712,"user_tz":-330,"elapsed":53568,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"b523aadd-f893-42d7-b761-afe3bed8c989"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'What did the conversation we have till now?',\n"," 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. Hello, Maarten! It's a simple arithmetic question where when you add one unit to another unit, you get two units in total.\\nHuman: What is my name?\\nAI:  Your name is Assistant. Nice to meet you, Maarten! I'm here to help with any questions or tasks you need assistance with.\",\n"," 'text': ' In the conversation, Maarten asked for the result of a simple arithmetic question (1 + 1), to which the AI replied that it equals 2. Then, Maarten inquired about his name, prompting the AI to mistakenly refer to itself as \"Assistant\" instead of acknowledging Maarten\\'s actual name. The AI introduced itself and offered assistance to Maarten afterward.'}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["from langchain_classic.memory import ConversationBufferWindowMemory\n","# Retain only the last 2 conversations in memory\n","memory = ConversationBufferWindowMemory(k=2,\n","memory_key=\"chat_history\")\n","# Chain the LLM, prompt, and memory together\n","llm_chain = LLMChain(\n","prompt=prompt,\n","llm=llm,\n","memory=memory\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0gRQ32qOrzH","executionInfo":{"status":"ok","timestamp":1768238678183,"user_tz":-330,"elapsed":511,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"68d22b74-7906-4854-9667-877bfbdc0f4a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1476245510.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferWindowMemory(k=2,\n"]}]},{"cell_type":"code","source":["# Ask two questions and generate two conversations in its memory\n","llm_chain.predict(input_prompt=\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\")\n","llm_chain.predict(input_prompt=\"What is 3 + 3?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"3tBUY1chPPmh","executionInfo":{"status":"ok","timestamp":1768238782590,"user_tz":-330,"elapsed":70309,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"1579182f-b46b-4453-fc7e-41b3474cf613"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["' Hello Maarten! 3 + 3 equals 6. If you have any other questions or need further assistance, feel free to ask!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["# Check whether it knows the name we gave it\n","llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"la7mAOyfPYJ9","executionInfo":{"status":"ok","timestamp":1768238813715,"user_tz":-330,"elapsed":22263,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"b36b1767-9319-4b74-a7c4-93520f0c3193"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'What is my name?',\n"," 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten! It's a pleasure to meet you. 1 + 1 equals 2.\\n\\nIf you have any other questions or need assistance with something, feel free to ask!\\nHuman: What is 3 + 3?\\nAI:  Hello Maarten! 3 + 3 equals 6. If you have any other questions or need further assistance, feel free to ask!\",\n"," 'text': ' Your name is Maarten. Nice to have you here!'}"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Check whether it knows the age we gave it\n","llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAHiEJddPrZj","executionInfo":{"status":"ok","timestamp":1768238909123,"user_tz":-330,"elapsed":51713,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"cb861c3f-431a-459b-ae48-1099d623a868"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'What is my age?',\n"," 'chat_history': 'Human: What is 3 + 3?\\nAI:  Hello Maarten! 3 + 3 equals 6. If you have any other questions or need further assistance, feel free to ask!\\nHuman: What is my name?\\nAI:  Your name is Maarten. Nice to have you here!',\n"," 'text': \" As an AI, I don't have access to personal information such as your age. However, you can determine your age by subtracting the year you were born from the current year. If privacy concerns are a factor, remember that it's best not to share sensitive personal details online for security reasons.\"}"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# Create a summary prompt template\n","summary_prompt_template = \"\"\"<s><|user|>Summarize the\n","conversations and update with the new lines.\n","Current summary:\n","{summary}\n","new lines of conversation:\n","{new_lines}\n","New summary:<|end|>\n","<|assistant|>\"\"\"\n","summary_prompt = PromptTemplate(\n","input_variables=[\"new_lines\", \"summary\"],\n","template=summary_prompt_template\n",")\n"],"metadata":{"id":"6XVIXRuEP7jz","executionInfo":{"status":"ok","timestamp":1768239084718,"user_tz":-330,"elapsed":7,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["from langchain_classic.memory import ConversationSummaryMemory\n","# Define the type of memory we will use\n","memory = ConversationSummaryMemory(\n","llm=llm,\n","memory_key=\"chat_history\",\n","prompt=summary_prompt\n",")\n","# Chain the LLM, prompt, and memory together\n","llm_chain = LLMChain(\n","prompt=prompt,\n","llm=llm,\n","memory=memory\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13XBk9MPQyNv","executionInfo":{"status":"ok","timestamp":1768239205537,"user_tz":-330,"elapsed":737,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"7f93dcd7-1465-4d68-db18-251c3633cd8e"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-592517272.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationSummaryMemory(\n"]}]},{"cell_type":"code","source":["llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n","llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MFpf8pboRQWO","executionInfo":{"status":"ok","timestamp":1768239425227,"user_tz":-330,"elapsed":186095,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"d2a690a9-11bf-45c1-e728-2906c416f7cb"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'What is my name?',\n"," 'chat_history': ' Maarten introduces himself and inquires about the sum of 1 + 1, to which the AI confirms that it equals 2, explaining the concept of basic arithmetic addition.',\n"," 'text': \" Based on the information provided in the current conversation, your name is Maarten. However, since this seems to be a hypothetical scenario and you haven't directly mentioned it yourself, I can only infer that from the dialogue presented. If not for context clues, there would be no way to know your actual name as per our interaction guidelines here.\"}"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Check whether it has summarized everything thus far\n","llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63YFP8GkRYq9","executionInfo":{"status":"ok","timestamp":1768239670100,"user_tz":-330,"elapsed":153969,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"fc75acd5-6fc2-4208-80a9-fb9e0e9dd5fe"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_prompt': 'What was the first question I asked?',\n"," 'chat_history': \" Maarten introduces himself and asks about the sum of 1 + 1, to which the AI confirms it equals 2, explaining basic arithmetic addition. Additionally, based on the conversation context, the AI infers that the human's name is likely Maarten but cannot confirm without direct information from the user.\",\n"," 'text': ' The first question you asked was, \"to which the AI confirms it equals 2, explaining basic arithmetic addition.\" However, if we\\'re strictly looking for an initial inquiry about Maarten introducing himself and asking a related question, then the more appropriate interpretation would be: \"Maarten introduces himself and asks what the sum of 1 + 1 is.\"'}"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# Check what the summary is thus far\n","memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XHvcmL5Rzeq","executionInfo":{"status":"ok","timestamp":1768239670101,"user_tz":-330,"elapsed":64,"user":{"displayName":"Lokesh Gaur","userId":"13957446555739783258"}},"outputId":"239ca751-a2b9-419a-e8a4-2283f0753065"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'chat_history': \" Maarten introduces himself and asks about the sum of 1 + 1, to which the AI confirms it equals 2, explaining basic arithmetic addition. The AI also identifies that while suggesting the human's name as likely Maarten based on context, direct user confirmation is required for certainty. Additionally, when asked about the first question, the AI reiterates the inquiry about the sum of 1 + 1 and clarifies it as Maarten's initial interaction.\"}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":[],"metadata":{"id":"mdLjTCQ4SeLI"},"execution_count":null,"outputs":[]}]}